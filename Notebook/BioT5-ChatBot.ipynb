{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.10.0\n",
      "GPUs: 5\n"
     ]
    }
   ],
   "source": [
    "# --- BIO T5 TRAINING (Robust Single GPU) ---\n",
    "# Uses a Custom Loop to bypass 'model.fit' and 'transformers' library bugs on Windows.\n",
    "# This is the guaranteed way to run without AttributeErrors.\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Mixed Precision (Disabled for Stability - Fixes NaN Loss)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MODEL_NAME = \"QizhiPei/biot5-base\"\n",
    "    CSV_PATH = r'D:\\AIvolution\\data\\qa_dataset_concatenated.csv'\n",
    "    MAX_LENGTH = 256\n",
    "    BATCH_SIZE = 4 \n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 1e-4\n",
    "    CHECKPOINT_DIR = r'D:\\AIvolution\\transformer\\biot5_checkpoints'\n",
    "    \n",
    "config = Config()\n",
    "if not os.path.exists(config.CHECKPOINT_DIR):\n",
    "    os.makedirs(config.CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data...\")\n",
    "df = pd.read_csv(config.CSV_PATH)\n",
    "df['question'] = df['question'].apply(lambda x: str(x).encode('ascii', 'ignore').decode('ascii'))\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x).encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "df[\"input_text\"] = \"question: \" + df[\"question\"]\n",
    "df[\"target_text\"] = df[\"answer\"]\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"input_text\", \"target_text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n",
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147600/147600 [04:03<00:00, 605.01 examples/s]\n",
      "Map (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16401/16401 [00:39<00:00, 416.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples, tokenizer=None, max_length=256):\n",
    "    model_inputs = tokenizer(examples[\"input_text\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target_text\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    fn_kwargs={'tokenizer': tokenizer, 'max_length': config.MAX_LENGTH},\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Model...\")\n",
    "try:\n",
    "    model = TFT5ForConditionalGeneration.from_pretrained(config.MODEL_NAME)\n",
    "except:\n",
    "    model = TFT5ForConditionalGeneration.from_pretrained(config.MODEL_NAME, from_pt=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, config.CHECKPOINT_DIR, max_to_keep=2)\n",
    "\n",
    "# Datasets\n",
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Manual Loss Function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def custom_loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Robust Training... ðŸš€\n",
      "\n",
      "Epoch 1/3\n",
      "36890/36900 [============================>.] - ETA: 3s - loss: 2.6549Saved checkpoint to D:\\AIvolution\\transformer\\biot5_checkpoints\\ckpt-1\n",
      "\n",
      "Epoch 2/3\n",
      "36890/36900 [============================>.] - ETA: 3s - loss: 2.0885Saved checkpoint to D:\\AIvolution\\transformer\\biot5_checkpoints\\ckpt-2\n",
      "\n",
      "Epoch 3/3\n",
      "36890/36900 [============================>.] - ETA: 3s - loss: 1.8929Saved checkpoint to D:\\AIvolution\\transformer\\biot5_checkpoints\\ckpt-3\n",
      "Saving Final Model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    # 1. Unpack Data correctly\n",
    "    if isinstance(inputs, (tuple, list)):\n",
    "        features, labels = inputs\n",
    "        input_ids = features['input_ids']\n",
    "        attention_mask = features['attention_mask']\n",
    "    else:\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        labels = inputs['labels']\n",
    "    \n",
    "    # 2. Shift labels for Decoder Input (Standard T5)\n",
    "    # decoder_input_ids = [pad] + labels[:-1]\n",
    "    start_token = 0 \n",
    "    shape = tf.shape(labels)\n",
    "    start_tokens = tf.fill([shape[0], 1], tf.cast(start_token, labels.dtype))\n",
    "    sliced_labels = labels[:, :-1]\n",
    "    decoder_input_ids = tf.concat([start_tokens, sliced_labels], axis=1)\n",
    "    \n",
    "    # 3. Forward Pass & Backprop\n",
    "    with tf.GradientTape() as tape:\n",
    "        # CRITICAL: Do NOT pass 'labels' here. It causes the library to crash.\n",
    "        # We calculate loss ourselves.\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            training=True\n",
    "        )\n",
    "        loss = custom_loss(labels, outputs.logits)\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# --- TRAINING EXECUTION ---\n",
    "steps_per_epoch = len(tokenized_datasets[\"train\"]) // config.BATCH_SIZE\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "print(\"Starting Robust Training... ðŸš€\")\n",
    "for epoch in range(config.EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n",
    "    train_loss_metric.reset_state()\n",
    "    progbar = tf.keras.utils.Progbar(steps_per_epoch)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tf_train_dataset):\n",
    "        loss = train_step(batch)\n",
    "        train_loss_metric.update_state(loss)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            progbar.update(batch_idx, values=[(\"loss\", train_loss_metric.result())])\n",
    "            \n",
    "    save_path = ckpt_manager.save()\n",
    "    print(f\"Saved checkpoint to {save_path}\")\n",
    "    \n",
    "print(\"Saving Final Model...\")\n",
    "model.save_pretrained(os.path.join(config.CHECKPOINT_DIR, 'final_model'))\n",
    "tokenizer.save_pretrained(os.path.join(config.CHECKPOINT_DIR, 'final_model'))\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
